---
title: 'Parallel Computing'
author: 'Jonathan Chipman with content provided by George Vega Yon'
format: 
  html:
    toc: TRUE
    toc-depth: 2
    toc-location: left
    highlight: pygments
    font_adjustment: -1
    css: styles.css
    # code-fold: true
    code-tools: true
    smooth-scroll: true
    embed-resources: true
---


# Introduction

Today's content has been drawn from:

  1. [https://github.com/gvegayon/ocrug-hpc-august2019](https://github.com/gvegayon/ocrug-hpc-august2019)
  
  2. Roger Peng's book: [R Programming for data science, Ch 22](https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html)
  
  3. Norman Matloff's book: [The Art of R Programming, Ch 16](https://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf#page=311)

# High-Performance Computing: An overview

Loosely, from R's perspective, we can think of HPC in terms of two, maybe three things:

1.  Big data: How to work with data that doesn't fit your computer

2.  Parallel computing: How to take advantage of multiple core systems

3.  Compiled code: Write your own low-level code (if R doesn't have it yet...)

(Checkout [CRAN Task View on HPC](https://cran.r-project.org/web/views/HighPerformanceComputing.html))


## Some vocabulary for HPC

In raw terms

*   Supercomputer: A **single** big machine with thousands of cores/gpus.

*   High Performance Computing (HPC): **Multiple** machines within
    a **single** network.
    
*   High Throughput Computing (HTC): **Multiple** machines across **multiple**
    networks.
    
You may not have access to a supercomputer, but certainly HPC/HTC clusters are
more accessible these days, e.g. AWS provides a service to create HPC clusters
at a low cost

## What's "a core"?

![Taxonomy of CPUs (Downloaded from de https://slurm.schedmd.com/mc_support.html)](fig/cpu-slurm.png){width="400px"}

Now, how many cores does your computer has, the parallel package can tell you that:

```{r 03-how-many-cores}
parallel::detectCores()
```

## What is parallel computing, anyway?

```r
f <- function(n) n*2
f(1:4)
```

![Here we are using a single core. The function is applied one element at a time, leaving the other 3 cores without usage.](fig/pll-computing-explained-serial.svg){width="50%"}


```r
f <- function(n) n*2
f(1:4)
```

![In this more intelligent way of computation, we are taking full advantage of our computer by using all 4 cores at the same time. This will translate in a reduced computation time which, in the case of complicated/long calculations, can be an important speed gain.](fig/pll-computing-explained-parallel.svg){width="50%"}


## When is it a good idea?

Let's think before we start...

![](https://media.giphy.com/media/Dwclsfe6Gb91m/giphy.gif){style="width:500px"}


> "In almost any parallel-processing application, you encounter overhead, or “wasted” time spent on noncomputational activity." [Matloff, ch 16, pg 337](https://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf#page=363).  See [16.4](https://diytranscriptomics.com/Reading/files/The%20Art%20of%20R%20Programming.pdf#page=371) for sources of overhead.

```{r good-idea, echo=FALSE, fig.cap="Ask yourself these questions before jumping into HPC!", fig.align='center', out.width="60%"}
knitr::include_graphics("fig/when_to_parallel.svg")
```

# Parallel computing in R

While there are several alternatives (just take a look at the
[High-Performance Computing Task View](https://cran.r-project.org/web/views/HighPerformanceComputing.html)),
we'll focus on the following R-packages for **explicit parallelism**.

> *   [**parallel**](https://cran.r-project.org/package=parallel): R package that provides '[s]upport for parallel computation,
    including random-number generation'.

> *   [**foreach**](https://cran.r-project.org/package=foreach): R package for 'general iteration over elements' in parallel fashion.

> *   [**future**](https://cran.r-project.org/package=future): '[A] lightweight and
    unified Future API for sequential and parallel processing of R
    expression via futures.' (won't cover here)
    
**Implicit parallelism or hidden parallelism**, on the other hand, are out-of-the-box tools that allow the programmer not to worry about parallelization, e.g. such as
[**gpuR**](https://cran.r-project.org/package=gpuR) for Matrix manipulation using
GPU, [**tensorflow**](https://cran.r-project.org/package=tensorflow), and [**data.table**](https://cran.r-project.org/web/packages/data.table/index.html)


And there's also a more advanced set of options

> *   [**Rcpp**](https://cran.r-project.org/package=Rcpp) + [OpenMP](https://www.openmp.org):
    [Rcpp](https://cran.r-project.org/package=Rcpp) is an R package for integrating
    R with C++, and OpenMP is a library for high-level parallelism for C/C++ and
    Fortran.

> *   A ton of other type of resources, notably the tools for working with 
    batch schedulers such as Slurm, HTCondor, etc.
    
# The parallel package

*  Based on the `snow` (simple network of workstations) and `multicore` R Packages.

*  Explicit parallelism.

*  Simple yet powerful idea: Parallel computing as multiple R sessions.

*  Clusters can be made of both local and remote sessions

*  Multiple types of cluster: `PSOCK`, `Fork`, `MPI`, etc. 

  > [Differences between socket and fork](https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/)

  > "For the fork, each parallel thread is a complete duplication of the master process with the shared environment, including objects or variables defined prior to the kickoff of parallel threads.         Therefore, it runs fast. However, the major limitation is that the fork doesn’t work on the         Windows system."
  
  > "On the other hand, the socket works on all operating systems. Each thread runs separately without sharing objects or variables, which can only be passed from the master process explicitly. As a result, it runs slower due to the communication overhead."

<div style="text-align: center;"><img src="fig/parallel-package.svg"/></div>

# Parallel workflow

(Usually) We do the following:

1.  Create a `PSOCK/FORK` (or other) cluster using `makePSOCKCluster`/`makeForkCluster`
    (or `makeCluster`)
    
2.  Copy/prepare each R session (if you are using a `PSOCK` cluster):

    a.  Copy objects with `parallel::clusterExport`

    b.  Pass expressions with `parallel::clusterEvalQ`

    c.  Set a seed

3.  Do your call: `parallel::parApply`, `parallel::parLapply`, etc. 

4.  Stop the cluster with `parallel::clusterStop`


[parallel::clusterApply documentation](https://stat.ethz.ch/R-manual/R-devel/library/parallel/html/clusterApply.html)

[parallel documentation](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf)


# Ex 1: Hello world!

```{r parallel-ex-psock, echo=TRUE, cache=TRUE}
# 1. CREATING A CLUSTER
library(parallel)
cores <- parallel::detectCores()
cl <- parallel::makePSOCKcluster(cores)    
x  <- 20

# 2. PREPARING THE CLUSTER
parallel::clusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`
parallel::clusterExport(cl, "x")

# 3. DO YOUR CALL
parallel::clusterEvalQ(cl, {
  paste0("Hello from process #", Sys.getpid(), ". I see x and it is equal to ", x)
})

# 4. STOP THE CLUSTER
parallel::stopCluster(cl)
```


# Ex 2: Parallel regressions

**Problem**: Run multiple regressions for each column of a very wide dataset. Report the $\beta$ coefficients from fitting the following regression models:

$$
y = X_{i}\beta_i + \varepsilon_{i},\quad \varepsilon_{i} \sim N(0, \sigma^2_i)
$$

$$
\quad\forall \text{ columns } i \in \left\{1, \dots, 999 \right\}
$$

```{r lots-of-lm-dgp, echo=TRUE}
set.seed(131)
y <- rnorm(500)
X <- matrix(rnorm(500*999), nrow = 500, dimnames = list(1:500, sprintf("x%03d", 1:999)))
```

```{r lots-of-lm-print}
dim(X)
X[1:6, 1:5]
str(y)
```

**Serial solution**:

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Use `apply` (for loop) to solve it

```{r lots-of-lm-serial, cache = TRUE, strip.white=FALSE}


ans <- apply(
  X      = X,
  MARGIN = 2,
  FUN    = function(x, y) coef(lm(y ~ x)),
  y      = y
  )

ans[,1:5]
```

# Ex 2: Parallel regressions (cont'd 2)

.

.

.

.

.

.

.

.

.

.

.

.

.

.



**Parallel solution**: Use `parApply`

```{r lots-of-lm-parallel, cache = TRUE}
library(parallel)
cl <- makePSOCKcluster(4L)
ans <- parApply(
  cl     = cl,
  X      = X,
  MARGIN = 2,
  FUN    = function(x, y) coef(lm(y ~ x)),
  y      = y
  )

ans[,1:5]
```

-----

Are we going any faster?  Compare the timing using the `bench` package.


.

.

.

.

.

.

.

.

.

.

.

.

.

.

```{r lots-of-lm-benchmark, cache = TRUE, warning=FALSE}
library(bench)
mark(
  parallel = parApply(
    cl  = cl,
    X   = X, MARGIN = 2,
    FUN = function(x, y) coef(lm(y ~ x)),
    y   = y
    ),
  serial = apply(
    X   = X, MARGIN = 2,
    FUN = function(x, y) coef(lm(y ~ x)),
    y   = y
    )
)
```

```{r lots-of-lm-stopcluster, cache = TRUE, echo=FALSE}
stopCluster(cl)
```


# How can you create a general purpose parallel computing package specific to OS?

There may be smarter ways, though this is how I approached it: [SeqSGPV package](https://github.com/chipmanj/SeqSGPV/blob/master/R/SeqSGPVrules.R)





# See also

*   [Blog example](http://gradientdescending.com/simple-parallel-processing-in-r/)
*   [Package parallel](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf) 
*   [Using the iterators package](https://cran.r-project.org/web/packages/iterators/vignettes/iterators.pdf)
*   [Using the foreach package](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf)
*   [32 OpenMP traps for C++ developers](https://software.intel.com/en-us/articles/32-openmp-traps-for-c-developers)
*   [The OpenMP API specification for parallel programming](http://www.openmp.org/)
*   ['openmp' tag in Rcpp gallery](gallery.rcpp.org/tags/openmp/)
*   [OpenMP tutorials and articles](http://www.openmp.org/resources/tutorials-articles/)

For more, checkout the [CRAN Task View on HPC](https://cran.r-project.org/web/views/HighPerformanceComputing.html){target="_blank"}

# Bonus track: Simulating $\pi$


*   We know that $\pi = \frac{A}{r^2}$. We approximate it by randomly adding
    points $x$ to a square of size 2 centered at the origin.

*   So, we approximate $\pi$ as $\Pr\{\|x\| \leq 1\}\times 2^2$

```{r, echo=FALSE, dev='jpeg', dev.args=list(quality=100), fig.width=6, fig.height=6, out.width='300px', out.height='300px'}
set.seed(1231)
p    <- matrix(runif(5e3*2, -1, 1), ncol=2)
pcol <- ifelse(sqrt(rowSums(p^2)) <= 1, adjustcolor("blue", .7), adjustcolor("gray", .7))
plot(p, col=pcol, pch=18)
```

#

The R code to do this

```{r simpi, echo=TRUE}
pisim <- function(i, nsim) {  # Notice we don't use the -i-
  # Random points
  ans  <- matrix(runif(nsim*2), ncol=2)
  
  # Distance to the origin
  ans  <- sqrt(rowSums(ans^2))
  
  # Estimated pi
  (sum(ans <= 1)*4)/nsim
}
```

#

```{r parallel-ex2, echo=TRUE, cache=TRUE}
library(parallel)
# Setup
cl <- makePSOCKcluster(4L)
clusterSetRNGStream(cl, 123)

# Number of simulations we want each time to run
nsim <- 1e5

# We need to make -nsim- and -pisim- available to the
# cluster
clusterExport(cl, c("nsim", "pisim"))

# Benchmarking: parSapply and sapply will run this simulation
# a hundred times each, so at the end we have 1e5*100 points
# to approximate pi
rbenchmark::benchmark(
  parallel = parSapply(cl, 1:100, pisim, nsim=nsim),
  serial   = sapply(1:100, pisim, nsim=nsim), replications = 1
)[,1:4]

```


# Not yet discussed: RcppArmadillo and OpenMP

*   Friendlier than [**RcppParallel**](http://rcppcore.github.io/RcppParallel/)...
    at least for 'I-use-Rcpp-but-don't-actually-know-much-about-C++' users (like myself!).

*   Must run only 'Thread-safe' calls, so calling R within parallel blocks can cause
    problems (almost all the time).
    
*   Use `arma` objects, e.g. `arma::mat`, `arma::vec`, etc. Or, if you are used to them
    `std::vector` objects as these are thread safe.

*   Pseudo-Random Number Generation is not very straight forward... But C++11 has
    a [nice set of functions](http://en.cppreference.com/w/cpp/numeric/random) that can be used together with OpenMP

*   Need to think about how processors work, cache memory, etc. Otherwise you could
    get into trouble... if your code is slower when run in parallel, then you probably
    are facing [false sharing](https://software.intel.com/en-us/articles/avoiding-and-identifying-false-sharing-among-threads)
    
*   If R crashes... try running R with a debugger (see
    [Section 4.3 in Writing R extensions](https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Checking-memory-access)):
    
    ```shell
    ~$ R --debugger=valgrind
    ```

# Session info
```{r}
devtools::session_info()
```
