---
title: "Functions and data.table"
subtitle: "PHS 7045: Advanced Programming"
format:
  revealjs:
    embed-resources: true
    slide-number: true
    scrollable: true
author: "George G. Vega Yon, Ph.D."
---

## Today's goals

```{r}
#| echo: false
#| label: setup
#| include: false
knitr::opts_chunk$set(eval = TRUE, results = "hold", fig.width = 7, fig.height = 5)
slides_eval <- TRUE
```


::: {.r-fit-text}

1. Discuss some technical aspects of functions in R.

2. We will learn about how to manipulate data with `data.table`, and in particular,

  - Selecting variables, filtering data, creating variables, and summarizing data.

Throughout the session, we will see examples using: 

- [**data.table**](https://cran.r-project.org/package=data.table) in R,
- [**dtplyr**](https://cran.r-project.org/package=dplyr) in R, and
- [**pydatatable**](https://github.com/h2oai/datatable)

Using the [MET](https://github.com/USCbiostats/data-science-data/) dataset.

:::


# Functions (part II)


## Functions: Scoping and environments

::: {.r-fit-text}

![Search paths and scoping (source: [Hadley Wickham's Advanced R]("Scoping rules https://adv-r.hadley.nz/environments.html"))](fig/search-path.png){width=50%}

- Where the object was created matters.

- And the order also matters.

**Question:** Two R packages with the same function name, `degree`. Are these two equivalent?

```r
library(igraph)
library(sna)
degree(...)
```

vs

```r
library(sna)
library(igraph)
degree(...)
```


::: {.fragment .fade-up}
What should you do?
:::

:::

---

::: {.r-fit-text}

In this example, the object `a` lives where `fun` was created, the Global Environment (`.GlobalEnv`, see `?.GlobalEnv`.)

```{r}
#| label: scoping-example-1
#| echo: true
a <- 1
fun <- function() {
  return(a)
}

fun_sub <- function() {
  a <- pi # It doesn't change the result!
  return(fun())
}

fun_sub()
```

```{r}
#| echo: true
#| label: scoping-example-1-cont
a <- 2
fun_sub() # It gets the new value
```

::: {.fragment .fade-up}
Question: What is the danger of this?
:::

:::

---

::: {.r-fit-text}

Now, things change if `fun` is moved to another environment.

```{r}
#| echo: true
#| eval: true
#| label: scoping-example-3
fun_sub()
```

```{r}
#| echo: true
#| eval: true
#| label: scoping-example-3-cont
fun_sub <- function() {
  a <- pi
  environment(fun) <- environment()
  fun()
}
fun_sub()
```

:::

## Functions: Arguments and copy-on-modify

::: {.r-fit-text}
- R is **very** smart when it comes to saving space.

- Objects passed to a function are only copied if modified, the "copy-on-modify" concept.

- This is very powerful as it mixes passing objects by reference and by copy dynamically (see this [wiki entry](https://en.wikipedia.org/wiki/Evaluation_strategy)).

```{r}
#| echo: true
#| eval: true
#| label: example-copy-modify
bigdata <- 123; data.table::address(bigdata)
```

```{r}
#| echo: true
(function(a) {
  data.table::address(a)
})(bigdata)
```

```{r}
#| echo: true
# If we change it...
(function(a) {
  a <- a + 0 # The input is now modified!
  data.table::address(a)
})(bigdata)
```

::: {.fragment .fade-up}
**Note:** Functions that don't have a name are called ["anonymous functions."](https://en.wikipedia.org/wiki/Anonymous_function)
:::

:::

# Data Table

## Data wrangling in R

::: {.r-fit-text}

Overall, you will find the following approaches:

- **base R**: Use only base R functions.

- **dplyr**: Using "verbs".

- **data.table**: High-performing (ideal for large data)

- **dplyr + data.table = dtplyr**: High-performing + dplyr verbs.

Other methods involve, for example, using external tools such as [Spark](http://spark.apache.org/), [sparkly](https://cran.r-project.org/package=sparklyr).

We will be focusing on data.table because of [this](https://h2oai.github.io/db-benchmark/)

Take a look at this neat cheat sheet by
[Erik Petrovski](http://www.petrovski.dk/) [here](https://github.com/rstudio/cheatsheets/raw/master/datatable.pdf).

:::

## Selecting variables: Load the packages

```{r}
#| label: loading-packages
#| eval: true
#| echo: true
library(data.table)
library(dtplyr)
library(dplyr)
library(ggplot2)
```

The `dtplyr` R package translates `dplyr` (`tidyverse`) syntax to `data.table`, so that we can still use **the dplyr verbs** while at the same time leveraging the performance of `data.table`.

## Loading the data

::: {.r-fit-text}

The data that we will be using is an already processed version of the [MET dataset](https://github.com/USCbiostats/data-science-data/raw/master/02_met/). We can download (and load) the data directly in our session using the following commands:

```{r}
#| label: downloading-data
#| echo: true
# Where are we getting the data from
met_url <- "https://github.com/USCbiostats/data-science-data/raw/master/02_met/met_all.gz"

# Downloading the data to a tempfile (so it is destroyed afterwards)
# you can replace this with, for example, your own data:
# tmp <- tempfile(fileext = ".gz")
tmp <- "met.gz"

# We sould be downloading this, ONLY IF this was not downloaded already.
# otherwise is just a waste of time.
if (!file.exists(tmp)) {
  download.file(
    url      = met_url,
    destfile = tmp,
    # method   = "libcurl", timeout = 1000 (you may need this option)
  )
}
```

:::

---

Now we can load the data using the `fread()` function. In R

```{r}
#| label: reading-data
#| eval: true
#| echo: true
# Reading the data
dat <- fread(tmp)
head(dat)
```

In Python

```{python reading-py, eval = TRUE, echo = TRUE, results='hide'}
import datatable as dt
dat = dt.fread("met.gz")
dat.head(5)
```

Before we continue, let's learn a bit more on `data.table` and `dtplyr`

---

## `data.table` and `dtplyr`: Data Table's Syntax

::: {.r-fit-text}

- As you have seen in previous lectures, in `data.table` all happens within the
square brackets. Here is common way to imagine DT:

<div align="center">
<img src="datatable-syntax-paths.svg" width="700px">
</div>

- Any time that you see **:=** in **j** that is "Assignment by reference." Using
  **=** within **j** only works in some specific cases.
  
:::

## `data.table` and `dtplyr`: Data Table's Syntax

Operations applied in **j** are evaluated *within* the data, meaning that
names work as symbols, e.g.,
  
```{r}
#| label: error-j
#| eval: false
#| echo: true
data(USArrests)
USArrests_dt <- data.table(USArrests)

# This returns an error
USArrests[, Murder]

# This works fine
USArrests_dt[, Murder]
```

---

Furthermore, we can do things like this:
  
```{r}
#| label: load-usarrests
#| echo: false
data("USArrests")
USArrests_dt <- data.table(USArrests)
```
  

```{r}
#| label: plot-usarrests
#| echo: true
USArrests_dt[, plot(Murder, UrbanPop)]
```

## `data.table` and `dtplyr`: Lazy table

- The `dtplyr` package provides a way to translate `dplyr` verbs to `data.table` syntax.

- The key lies on the function `lazy_dt` from `dtplyr` (see `?dtplyr::lazy_dt`).

- This function creates a wrapper that "points" to a `data.table` object


## `data.table` and `dtplyr`: Lazy table (cont.) {.smaller}


```{r}
#| label: lazy-dt
#| echo: true
# Creating a lazy table object
dat_ldt <- lazy_dt(dat, immutable = FALSE)

# We can use the address() function from data.table
address(dat)
address(dat_ldt$parent)

# To ensure we are not modifying it
# SEEMS THERES A BUG HERE!
dat_ldt <- lazy_dt(dat, immutable = TRUE) 
```

![](pointer-paths.svg){width="800px"}

Question: What is the `immutable = FALSE` option used for?


## Selecting columns

::: {.r-fit-text}

How can we select the columns `USAFID`, `lat`, and `lon`, using `data.table`:

```{r}
#| label: select1-dt
#| eval: !expr slides_eval
#| echo: true
dat[, list(USAFID, lat, lon)]
# dat[, .(USAFID, lat, lon)]       # Alternative 1
# dat[, c("USAFID", "lat", "lon")] # Alternative 2
```

What happens if instead of `list()` you used `c()`?

:::


## Selecting columns (cont. 1) 

Using the **dplyr::select** verb:

```{r}
#| label: select1-tv
#| eval: !expr slides_eval
#| echo: true
dat_ldt |>  select(USAFID, lat, lon)
```

## Selecting columns (cont. 2) {.small}

In the case of `pydatatable`

```{python}
#| label: select1-py
#| eval: !expr slides_eval
#| echo: true
dat[:,["USAFID", "lat", "lon"]]
```

::: {style="font-size: 20pt"}

What happens if instead of `["USAFID", "lat", "lon"]` you used `{"USAFID", "lat", "lon"}` (vector vs set).

:::


---

For the rest of the session, we will use these variables: USAFID, WBAN, year, month, day, hour, min, lat, lon, elev, wind.sp, temp, and atm.press.

```{r}
#| label: actual-selection
#| echo: true
# Data.table
cat(dat[1,] |> colnames(), file="tmp.tmp")
dat <- dat[,
  .(USAFID, WBAN, year, month, day, hour, min, lat, lon, elev,
    wind.sp, temp, atm.press)]

# Need to redo the lazy table
dat_ldt <- lazy_dt(dat)
```


## Data filtering: Logical conditions 

::: {style="font-size: 20pt"}

- Based on logical operations, *e.g.*, `condition 1 [and|or condition2 [and|or ...]]`

- Need to be aware of ordering and grouping of `and` and `or` operators (see `Comparison`).
  
  ```{r}
  #| label: operators
  #| echo: false
  test <- expand.grid(c(TRUE, FALSE),c(TRUE, FALSE))
  ans <- data.frame(
    x = ifelse(test[,1], "true", "false"),
    y = ifelse(test[,2], "true", "false"),
    `Negate<br>!x`       = ifelse(!test[,1] , "true", "false"),
    `And<br>x & y`       = ifelse(test[,1] & test[,2], "true", "false"),
    `Or<br>x | y`        = ifelse(test[,1] | test[,2], "true", "false"),
    `Xor<br>xor(x, y)`       = ifelse(xor(test[,1], test[,2]), "true", "false"),
    check.names     = FALSE
  )
  
  knitr::kable(ans, row.names = FALSE, align = "c")
  ```

:::

## Questions 1: How many ways can you write an XOR operator? 

Write a function that takes two arguments `(x,y)` and applies the XOR operator
element-wise. Here you have a template:

```r
myxor <- function(x, y) {
  res <- logical(length(x))
  for (i in 1:length(x)) {
    res[i] <- # do something with x[i] and y[i]
  }
  return(res)
}
```

Or if vectorized (which would be better)

<!-- ```r
myxor <- function(x, y) {
  # INSERT YOUR CODE HERE
}
``` -->

::: {style="font-size: 20pt"}

Hint 1: Remember that negating `(x & y)` equals `(!x | !y)`.

Hint 2: Logical operators are distributive, meaning
`a * (b + c) = (a * b) + (a + c)`, where `*` and `+` are `&` or `|`.

:::


---

::: {.r-fit-text}

In R

```{r}
#| label: xor-r
#| echo: true
myxor1 <- function(x,y) {(x & !y) | (!x & y)}
myxor2 <- function(x,y) {!((!x | y) & (x | !y))}
myxor3 <- function(x,y) {(x | y) & (!x | !y)}
myxor4 <- function(x,y) {!((!x & !y) | (x & y))}
cbind(
  ifelse(xor(test[,1], test[,2]), "true", "false"),
  ifelse(myxor1(test[,1], test[,2]), "true", "false"),
  ifelse(myxor2(test[,1], test[,2]), "true", "false"),
  ifelse(myxor3(test[,1], test[,2]), "true", "false"),
  ifelse(myxor4(test[,1], test[,2]), "true", "false")
)
```

:::

---

::: {.smaller}

Or in python

```{python}
#| label: xor-py
#| echo: true
# Loading the libraries
import numpy as np
import pandas as pa

# Defining the data
x = [True, True, False, False]
y = [False, True, True, False]
ans = {
    'x'   : x,
    'y'   : y,
    'and' : np.logical_and(x, y),
    'or'  : np.logical_or(x, y),
    'xor' : np.logical_xor(x, y)
    }
pa.DataFrame(ans)
```

:::

---

<!-- ::: {.r-fit-text} -->

Or in python (bis)

```{python}
#| label: xor-py-bis
#| echo: true
def myxor(x,y):
    return np.logical_or(
        np.logical_and(x, np.logical_not(y)),
        np.logical_and(np.logical_not(x), y)
    )

ans['myxor'] = myxor(x,y)
pa.DataFrame(ans)
```

We will now see applications using the `met` dataset.

<!-- ::: -->

## Filtering (subsetting) the data

::: {style="font-size: 20pt"}

Need to select records according to some criteria. For example:

- First day of the month, and
- Above latitude 40, and
- Elevation outside the range between 500 and 1,000.

The logical expressions would be

- `(day == 1)` 
- `(lat > 40)`
- `((elev < 500) | (elev > 1000))`

Respectively.

:::

---

In R with `data.table`:

```{r}
#| label: filter1-dt1
#| echo: true
#| eval: !expr slides_eval
dat[(day == 1) & (lat > 40) & ((elev < 500) | (elev > 1000))] %>%
  nrow()
```

---

In R with **dplyr::filter()**:

```{r}
#| label: filter1-tv
#| echo: true
#| eval: !expr slides_eval
dat_ldt %>%
  filter(day == 1, lat > 40, (elev < 500) | (elev > 1000)) %>%
  collect() %>% # Notice this line!
  nrow() 
```

---

::: {.r-fit-text}

In Python

```{python}
#| label: filter1-py-load
#| eval: !expr slides_eval
#| echo: true
import datatable as dt
dat = dt.fread("met.gz")
```


```{python}
#| label: filter1-py
#| eval: !expr slides_eval
#| echo: true
dat[(dt.f.day == 1) & (dt.f.lat > 40) & ((dt.f.elev < 500) | (dt.f.elev > 1000)), :].nrows
# dat[dt.f.day == 1,:][dt.f.lat > 40,:][(dt.f.elev < 500) | (dt.f.elev > 1000),:].nrows
```

In the case of pydatatable we use `dt.f.` to refer to a column. `df.` is what we use to refer to datatable's [namespace](https://en.wikipedia.org/wiki/Namespace).

The [`f.` is a symbol](https://datatable.readthedocs.io/en/latest/manual/f-expressions.html) that allows accessing column names in a datatable's `Frame`.

:::

## Questions 2

1. How many records have a temperature between 18 and 25?

2. Some records have missings. Count how many records have `temp` as `NA`?

3. Following the previous question, plot a sample of 1,000 of `(lat, lon)` of the
   stations with temp as NA and those with data.

---

## Solutions

```{r}
#| echo: false
#| eval: true
# Question 1
message("Question 1: ", nrow(dat[(temp < 25) & (temp > 18)]))
# dat[temp %between% c(18, 25), .N] 

# dat_ldt %>% filter(between(temp, 18, 25)) %>% collect() %>% nrow()

# Question 2
message("Question 2: ", dat[is.na(temp), .N])

# Question 3
set.seed(123)
message("Question 3")

# Drawing a sample
idx <- dat[, list(x = sample.int(.N, 2000, replace = FALSE)), by = is.na(temp)]$x

# Visualizing the data
ggplot(map_data("state"), aes(x = long, y = lat)) +
  geom_map(aes(map_id = region), map = map_data("state"), col = "lightgrey", fill = "gray") +
  geom_jitter(
    data    = dat[idx],
    mapping = aes(x = lon, y = lat, col = is.na(temp)),
    inherit.aes = FALSE, alpha = .5, cex = 2
    )

```


## Creating variables: Data types

::: {.r-fit-text}

- **logical**: Bool true/false type, *e.g.*, dead/alive, sick/healthy, good/bad, yes/no, etc.

- **strings**: string of characters (letters/symbols), *e.g.*, names, text, etc.

- **integer**: Numeric variable with no decimal (discrete), *e.g.*, age, days, counts, etc.

- **double**: Numeric variable with decimals (continuous), *e.g.*, distance, expression level, time.

In C (and other languages), strings, integers, and doubles may be specified with size, *e.g.*, in `python` integers can be of 9, 16, and 32 bits. This is relevant when managing large datasets, where saving space can be fundamental ([more info](https://en.wikipedia.org/wiki/C_data_types#Main_types)).

:::

---

## Creating variables: Special data types

::: {.r-fit-text}

Most programming languages have special types which are built using basic types. Here are a few examples:

- **time**: Could be date, date + time, or a combination of both. Usually, it has a reference number defined as date 0. In R, the `Date` class has as reference 1970-01-01, in other words, "days since January 1st, 1970".
  
- **categorical**: Commonly used to represent strata/levels of variables, *e.g.*, a variable "country" could be represented as a factor where the data is stored as numbers but has a label.
  
- **ordinal**: Similar to factor, but it has ordering, e.g.
  "satisfaction level: 5 very satisfied, ..., 1 very unsatisfied".

Other noteworthy data types could be ways to represent missings (usually described as `na` or `NA`), or particular numeric types, e.g., `+-Inf` and Undefined (`NaN`).


When storing/sharing datasets, it is a good practice to do it along a dictionary describing each column data type/format.

:::

## Questions 3: What's the best way to represent the following

::: {.smaller}

- 0, 1, 1, 0, 0, 1

- Diabetes type 1, Diabetes type 2, Diabetes type 1, Diabetes type 2

- on, off, off, on, on, on

- 5, 10, 1, 15, 0, 0, 1

- 1.0, 2.0, 10.0, 6.0

- high, low, medium, medium, high

- -1, 1, -1, -1, 1,

- .2, 1.5, .8, $\pi$

- $\pi$, $\exp{1}$, $\pi$, $\pi$

:::

---

## Variable creation

If we wanted to create two variables, `elev^2` and the scaled version of `wind.sp` by it's standard error, we could do the following:

Using `data.table`

```{r}
#| eval: !expr slides_eval
#| echo: true
dat[, elev2         := elev^2]
dat[, windsp_scaled := wind.sp/sd(wind.sp, na.rm = TRUE)]
# Alternatively:
# dat[, c("elev2", "windsp_scaled") := .(elev^2, wind.sp/sd(wind.sp,na.rm=TRUE)) ]
```

---

## Variable creation (cont. 1)

With the verb **dplyr::mutate()**:

```{r}
#| eval: !expr slides_eval
dat[, c("elev2", "windsp_scaled") := NULL] # This to delete these variables
dat_ldt %>%
  mutate(
    elev2         = elev ^ 2,
    windsp_scaled = wind.sp/sd(wind.sp,na.rm=TRUE)
  ) %>% collect()
```

Notice that since `dat_ldt` is a pointer, the variables are now in `dat`.


## Variable creation (cont. 2)

Imagine that we needed to generate all those calculations (scale by sd) on many more variables.
We could then use the **.SD** symbol:

```{r}
#| label: var-new-dt2
# Listing the names
in_names  <- c("wind.sp", "temp", "atm.press")
out_names <- paste0(in_names, "_scaled")
dat[,
    c(out_names) := lapply(.SD, function(x) x/sd(x, na.rm = TRUE)), 
    .SDcols = in_names
    ]

# Looking at the first 6
head(dat[, .SD, .SDcols = out_names], n = 4)
```

Key things to notice here: **c(out_names)**, **.SD**, and **.SDCols**.


## Variable creation (cont. 3)

In the case of dplyr, we could use the following

```{r}
#| label: var-new-tv2
as_tibble(dat_ldt) %>%
  mutate(
    across(
      all_of(in_names),
      function(x) x/sd(x, na.rm = TRUE),
      .names = "{col}_scaled2"
      )
  ) %>%
  # Just to print the last columns
  select(ends_with("_scaled2")) %>%
  head(n = 4)
```

A key thing here: This approach has no direct translation to `data.table`, which is why we used **as_tibble()**.


## Merging data

- While building the MET dataset, we dropped the State data.

- We can use the original Stations dataset and *merge* it with the MET dataset.

- But we cannot do it right away. We need to process the data somewhat first.


## Merging data (cont. 1)

::: {.r-fit-text}

```{r}
#| label: stations-data
#| warning: false
#| echo: true
stations <- fread("ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv")
stations[, USAF := as.integer(USAF)]

# Dealing with NAs and 999999
stations[, USAF   := fifelse(USAF == 999999, NA_integer_, USAF)]
stations[, CTRY   := fifelse(CTRY == "", NA_character_, CTRY)]
stations[, STATE  := fifelse(STATE == "", NA_character_, STATE)]

# Selecting the three relevant columns, and keeping unique records
stations <- unique(stations[, list(USAF, CTRY, STATE)])

# Dropping NAs
stations <- stations[!is.na(USAF)]

head(stations, n = 4)
```

Notice the function `fifelse()`. Now, let's try to merge the data!

:::


## Merging data (cont. 2)

::: {.r-fit-text}

```{r}
#| label: merge-try1
#| echo: true
merge(
  # Data
  x     = dat,      
  y     = stations, 
  # List of variables to match
  by.x  = "USAFID",
  by.y  = "USAF", 
  # Which obs to keep?
  all.x = TRUE,      
  all.y = FALSE, 
  ) %>% nrow()
```

This is more rows! The original dataset, `dat`, has `r nrow(dat)`. This means
that the `stations` dataset has duplicated IDs. We can fix this:

```{r}
#| label: drop-dpl
#| results: 'hide'
#| echo: true
stations[, n := 1:.N, by = .(USAF)]
stations <- stations[n == 1,][, n := NULL]

```

:::

## Merging data (cont. 3)

::: {.r-fit-text}

We can now use the function `merge()` to add the extra data

```{r}
#| label: merge
#| eval: true
#| echo: true
dat <- merge(
  # Data
  x     = dat,      
  y     = stations, 
  # List of variables to match
  by.x  = "USAFID",
  by.y  = "USAF", 
  # Which obs to keep?
  all.x = TRUE,      
  all.y = FALSE
  )

head(dat[, list(USAFID, WBAN, STATE)], n = 4)
```

:::

---

::: {.r-fit-text}

What happens when you change the options `all.x` and `all.y`?

```{r}
#| label: merging-all-x-y
#| include: false
merge(x = dat, y = stations, by.x = "USAFID", by.y = "USAF",all.x = TRUE, all.y = FALSE) %>% nrow()
merge(x = dat, y = stations, by.x = "USAFID", by.y = "USAF",all.x = TRUE, all.y = TRUE) %>% nrow()
merge(x = dat, y = stations, by.x = "USAFID", by.y = "USAF",all.x = FALSE, all.y = TRUE) %>% nrow()
merge(x = dat, y = stations, by.x = "USAFID", by.y = "USAF",all.x = FALSE, all.y = FALSE) %>% nrow()
```

:::

---

## Aggregating data: Adding groupped variables

- Often, we must either impute some data or generate variables by strata.

- If we, for example, wanted to impute missing temperature with the daily state
  average, we could use **by** together with the **data.table::fcoalesce()** function:
  
  ```{r}
  #| label: group-dt1
  #| eval: true
  dat[, temp_imp := fcoalesce(temp, mean(temp, na.rm = TRUE)),
    by = .(STATE, year, month, day)]
  ```

- In the case of dplyr, we can do the following using **dplyr::group_by** together
  with **dplyr::coalesce()**:
  
  ```{r}
  #| label: group-tv1
  #| eval: true
  #| results: 'hide'
  # We need to create the lazy table again, since we replaced it in the merge
  dat_ldt <- lazy_dt(dat, immutable = FALSE)
  
  dat_ldt %>%
    group_by(STATE, year, month, day) %>%
    mutate(
      temp_imp2 = coalesce(temp, mean(temp, na.rm = TRUE))
      ) %>% collect()
  ```

---

## Aggregating data: Adding grouped variables (cont.)

Let's see how it looks like

```{r}
#| label: plot-groupped
#| eval: false
# Preparing for ggplot2
plotdata <-dat[USAFID == 720172][order(year, month, day)]
plotdata <- rbind(
  plotdata[, .(temp = temp, type = "raw")],
  plotdata[USAFID == 720172][, .(temp = temp_imp, type = "filled")]
)

# Generating an 'x' variable for time
plotdata[, id := 1:.N, by = type]

plotdata %>%
  ggplot(aes(x = id, y = temp, col = type, lty = type)) +
  geom_line()
```

---

```{r}
#| echo: false
# Preparing for ggplot2
plotdata <-dat[USAFID == 720172][order(year, month, day)]
plotdata <- rbind(
  plotdata[, .(temp = temp, type = "raw")],
  plotdata[USAFID == 720172][, .(temp = temp_imp, type = "filled")]
)

# Generating an 'x' variable for time
plotdata[, id := 1:.N, by = type]

plotdata %>%
  ggplot(aes(x = id, y = temp, col = type, lty = type)) +
  geom_line()
```


---

## Aggregating data: Summary table

- Using `by` also allow us creating summaries of our data.

- For example, if we wanted to compute the average temperature, wind-speed, and
  atmospheric preassure by state, we could do the following
  
  ```{r}
  #| label: by-state-dt
  dat[, .(
    temp_avg      = mean(temp, na.rm=TRUE),
    wind.sp_avg   = mean(wind.sp, na.rm=TRUE),
    atm.press_avg = mean(atm.press, na.rm = TRUE)
    ),
    by = STATE
    ][order(STATE)] %>% head(n = 4)
  ```
  
---

## Aggregating data: Summary table (cont. 1)

When dealing with too many variables, we can use the `.SD` special symbol in `data.table`:

```{r}
#| label: group-dt2
#| eval: true
# Listing the names
in_names  <- c("wind.sp", "temp", "atm.press")
out_names <- paste0(in_names, "_avg")

dat[,
  setNames(lapply(.SD, mean, na.rm = TRUE), out_names),
  .SDcols = in_names, keyby   = STATE
  ] %>% head(n = 4)
```

Notice the **keyby** option here: "Group by STATE and order by STATE".

---

## Aggregating data: Summary table (cont. 2)

- Using **dplyr** verbs  
  
  ```{r}
  #| label: by-state-tv
  dat_ldt %>% group_by(STATE) %>%
    summarise(
      temp_avg      = mean(temp, na.rm=TRUE),
      wind.sp_avg   = mean(wind.sp, na.rm=TRUE),
      atm.press_avg = mean(atm.press, na.rm = TRUE)
    ) %>% arrange(STATE) %>% head(n = 4)
  ```
  
  Notice the `arrange()` function.

---

## Other data.table goodies

::: {.r-fit-text}

- `shift()` Fast lead/lag for vectors and lists.

- `fifelse()` Fast if-else, similar to base R's `ifelse()`.

- `fcoalesce()` Fast coalescing of missing values.

- `%between%` A short form of `(x < lb) & (x > up)`

- `%inrange%` A short form of `x %in% lb:up`

- `%chin%` Fast match of character vectors, equivalent to `x %in% X`,
  where both `x` and `X` are character vectors.
  
- `nafill()` Fill missing values using a constant, last observed value, or
  the next observed value.
  
:::

---

## Benchmarks

- [H2O.ai](https://www.h2o.ai/)'s benchmark ([link](https://h2oai.github.io/db-benchmark/)): Designed by
  the lead developer of data.table [Matt Dowle](https://rdatatable.gitlab.io/data.table/)

- [RStudio's](https://rstudio.com) benchmark ([link](https://cran.r-project.org/web/packages/vroom/vignettes/benchmarks.html)):
  Designed as part of the benchmarks with the [vroom](https://cran.r-package.org/package=vroom) package.


